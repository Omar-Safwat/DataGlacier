{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bab13b5",
   "metadata": {},
   "source": [
    "**Author:** Omar Safwat<br>\n",
    "**Date:** 2021-04-11<br>\n",
    "**Batch:** LISP01<br>\n",
    "\n",
    "# Week 6: File Ingestion and Schema Validation\n",
    "\n",
    "This notebook demonstrates the pipeline of automated data ingestion in typical day-to-day data science tasks. The code uses the **\"ddos_balanced\"** data set; a randomly selected file, that exceeds 5 GB in size for demonstration purposes. Data can be downloaded from [this link.](https://www.kaggle.com/devendra416/ddos-datasets)\n",
    "\n",
    "# File Ingestion\n",
    "## Summarizing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15890048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size: 6.794744782 GB\n",
      "Number of rows in file: 12794628\n"
     ]
    }
   ],
   "source": [
    "# Libraries of Reading in files\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from time import time # To monitor process time\n",
    "from os import stat # Line count\n",
    "from zipfile import ZipFile\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Extract the zip file containing the data set\n",
    "with ZipFile('final_dataset.csv.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()\n",
    "    \n",
    "# Print File size in GB and then count the number of lines\n",
    "print(f\"File Size: {stat('final_dataset.csv').st_size * 1e-09} GB\")\n",
    "with open(\"final_dataset.csv\", 'r') as file:\n",
    "    line_count = 0\n",
    "    for line in file:\n",
    "        if line != \"\\n\":\n",
    "            line_count += 1\n",
    "print(f\"Number of rows in file: {line_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e060be9",
   "metadata": {},
   "source": [
    "## Importing data\n",
    "\n",
    "The file was read with 2 methods; each method was timed to emphasize the importance of using the proper reading methods when reading files larger than 2 GB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6755fcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading with Dask tool:  0.3025329113006592  seconds\n",
      "Reading wth Pandas took:  96.82581210136414  seconds\n"
     ]
    }
   ],
   "source": [
    "# Reading the dataset with Dask\n",
    "start = time()\n",
    "df_dask = dd.read_csv(\"final_dataset.csv\")\n",
    "end = time()\n",
    "print(\"Reading with Dask took: \", end - start, \" seconds\")\n",
    "\n",
    "# Reading with pandas\n",
    "# read the large csv file with specified chunksize and append chunk to a single list\n",
    "start = time()\n",
    "chunk_list = []\n",
    "with pd.read_table(\"final_dataset.csv\", chunksize=500000, low_memory=False) as reader:\n",
    "    for chunk in reader:\n",
    "        chunk_list.append(chunk)\n",
    "df_concat = pd.concat(chunk_list)\n",
    "end = time()\n",
    "print(\"Reading wth Pandas took: \", end - start, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c428f4f",
   "metadata": {},
   "source": [
    "# Schema validation\n",
    "\n",
    "A YAML configuration file is created inorder to automate the file reading process in the future. The configuration file specifies the essential arguments used by reading methods, and contains the expected columns, this allows us to validate header names after reading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4592e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess column names\n",
    "df_dask.columns = df_dask.columns.str.replace('[#, @, $, %, &, !, :, ;]', '', regex=True)\n",
    "df_dask.columns = df_dask.columns.str.replace(' ', '_')\n",
    "df_dask.columns = df_dask.columns.str.lower()\n",
    "\n",
    "# Write Config file\n",
    "yaml_param = '''\n",
    "file_name: \"final_dataset.csv\"\n",
    "output_file: \"gzip_data\"\n",
    "output_format: \"gzip\"\n",
    "inbound_sep: ','\n",
    "outbound_sep: '|'\n",
    "rows_to_skip: 0\n",
    "table_name: \"df_dask\"\n",
    "'''\n",
    "\n",
    "with open('config.yml', \"w+\") as file:\n",
    "    param = yaml.full_load(yaml_param)\n",
    "    yaml.dump(param, file)\n",
    "\n",
    "with open('config.yml', \"a\") as file:\n",
    "    yaml.dump({'columns' : list(df_dask.columns)}, file, default_flow_style= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc85802",
   "metadata": {},
   "source": [
    "The code below creates two functions that read in the created YAML configuration file and use it to validate data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8bc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read config file\n",
    "def read_config(fileName):\n",
    "    with open(fileName, \"r\") as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    return(config)\n",
    "\n",
    "# Function to validate columns of data intake\n",
    "def validate_cols(data_cols, config_cols):\n",
    "    \"\"\"\n",
    "    Function args\n",
    "    -------------\n",
    "    data_cols: df.columns() # A pandas.series\n",
    "    config_cols: a list of column names from the configuration file\n",
    "    \"\"\"\n",
    "    data_cols = data_cols.str.replace('[#, @, $, %, &, !, :, ;]', '', regex=True)\n",
    "    data_cols = data_cols.str.replace(' ', '_')\n",
    "    data_cols = data_cols.str.lower()\n",
    "\n",
    "    # Validate that columns match in both lists.\n",
    "    if len(data_cols) == len(config_cols) and list(data_cols).sort() == config_cols.sort():\n",
    "        print(\"Columns validation was successful\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Columns validation has failed\")\n",
    "        missing_in_yaml = list(set(data_cols).difference(config_cols))\n",
    "        print('The following columns were not in YAML: ', missing_in_yaml)\n",
    "        missing_in_data = list(set(config_cols).difference(data_cols))\n",
    "        print('The following columns were not in your data: ', missing_in_data)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4bdecd",
   "metadata": {},
   "source": [
    "The configuration file is then read and used to validate the imported data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the configuration YAML file to validate the data imported \n",
    "configs = read_config(\"config.yml\")\n",
    "df = dd.read_table(configs['file_name'], sep=configs['inbound_sep'])\n",
    "validate_cols(df.columns, configs['columns'])\n",
    "      \n",
    "# Compress file and store it in gz format\n",
    "df.to_csv(configs['out_file'], sep=configs['outbound_sep'], compression=configs['output_format']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
